{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c8508604f47a28fba34c6fde12f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MVP-Code\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "write a python code implementing linked list\n",
      "assistant\n",
      "write a python code implementing linked list\n",
      "\n",
      "## 1.1.1.1\n",
      "\n",
      "## 1.1.1.2\n",
      "\n",
      "## 1.1.1.3\n",
      "\n",
      "## 1.1.1.4\n",
      "\n",
      "## 1.1.1.5\n",
      "\n",
      "## 1.1.1.6\n",
      "\n",
      "## 1.1.1.7\n",
      "\n",
      "## 1.1.1.8\n",
      "\n",
      "## 1\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"write a python code implementing linked list\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "# ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "\n",
    "ds_org = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_org['train'].train_test_split(test_size=0.2,seed=442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ds['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Write a Python program to decode the given encoded text using ROT-13 algorithm. \n",
      "*\n",
      "input: Gur pynff vf snpgbevnag \n",
      "*\n",
      "output: The code is obfuscated \n",
      "*\n",
      "*****\n",
      "instruction: Compose a program in Python to retrieve the most frequent letter in a given string. \n",
      "*\n",
      "input: String: \"Programmers\" \n",
      "*\n",
      "output: def most_frequent(input_string):\n",
      "    # Convert input string to a dictionary\n",
      "    char_frequency = {}\n",
      "    for n in input_string:\n",
      "        keys = char_frequency.keys()\n",
      "        if n in keys:\n",
      "            char_frequency[n] += 1\n",
      "        else:\n",
      "            char_frequency[n] = 1\n",
      "     \n",
      "    # Sort the dictionary by value in descending order\n",
      "    max_value = max(char_frequency.values())\n",
      "    max_char = [element for element in char_frequency if char_frequency[element] == max_value]\n",
      " \n",
      "    # Print the most frequent letter\n",
      "    if len(max_char) > 1:\n",
      "        print(\"Most frequent character: \" + \",\".join(max_char))\n",
      "    else:\n",
      "        print(\"Most frequent character: \" + max_char[0])\n",
      "\n",
      "most_frequent(\"Programmers\") \n",
      "*\n",
      "*****\n",
      "instruction: Create a Python program to take an array of numbers and calculate the average. \n",
      "*\n",
      "input: [1, 4, 5, 6, 7] \n",
      "*\n",
      "output: def average(nums):\n",
      "    sum = 0\n",
      "    for n in nums:\n",
      "        sum += n\n",
      "    return sum / len(nums)\n",
      "\n",
      "nums = [1, 4, 5, 6, 7]\n",
      "print(average(nums)) \n",
      "*\n",
      "*****\n",
      "instruction: Write a Python function to convert a given floating point number to binary. \n",
      "*\n",
      "input: 19.66 \n",
      "*\n",
      "output: def decimal_to_binary(f_num):\n",
      "  b_str = \"\"\n",
      "  int_num = int(f_num)\n",
      "  f_num = f_num - int_num\n",
      "\n",
      "  while int_num > 0:\n",
      "    b_str = str(int_num % 2) + b_str\n",
      "    int_num //= 2\n",
      "\n",
      "  b_str = \"1.\" + b_str\n",
      "\n",
      "  while f_num > 0: \n",
      "    f_num *= 2\n",
      "    int_num = int(f_num)\n",
      "    if int_num == 1:\n",
      "      b_str += \"1\"\n",
      "    else:\n",
      "      b_str += \"0\"\n",
      "    f_num = f_num - int_num\n",
      "\n",
      "  return b_str \n",
      "*\n",
      "*****\n",
      "instruction: Write a regex to identify all the words starting with 't' and ending with 'e' in a given string. \n",
      "*\n",
      "input: str1=\"this is a python program\" \n",
      "*\n",
      "output: import re\n",
      "\n",
      "def identify_words(str1):\n",
      "    regex = \"\\w*t\\w*e\\b\"\n",
      "    words = re.findall(regex, str1)\n",
      "    return words\n",
      "\n",
      "print(identify_words(\"this is a python program\")) \n",
      "*\n",
      "*****\n",
      "instruction: Design a class in Python with instance attributes for name, height, and age of a person. \n",
      "*\n",
      "input: Not applicable \n",
      "*\n",
      "output: class Person:\n",
      " def __init__(self, name, height, age):\n",
      " self.name = name\n",
      " self.height = height\n",
      " self.age = age \n",
      "*\n",
      "*****\n",
      "instruction: Build a REST API with Python and Flask. The API should include the following endpoints: POST /users, GET /users/{userId}, GET /users/{userId}/posts, POST /posts \n",
      "*\n",
      "input: Not applicable \n",
      "*\n",
      "output: # Importing libraries\n",
      "import flask\n",
      "from flask import Flask, request, jsonify\n",
      "\n",
      "# Creating the application instance\n",
      "app = Flask(__name__)\n",
      "\n",
      "# A list to store users data temporarily\n",
      "users = []\n",
      "\n",
      "@app.route('/users', methods=['POST'])\n",
      "def add_user():\n",
      " data = request.get_json()\n",
      " users.append(data)\n",
      " return jsonify({'message': 'User added successfully'})\n",
      "\n",
      "@app.route('/users/<userId>', methods=['GET'])\n",
      "def get_user(userId):\n",
      " user = [user for user in users if user['userId'] == userId]\n",
      " if len(user) == 0:\n",
      " return jsonify({'message': 'No user found'})\n",
      " return jsonify({'user': user[0]})\n",
      "\n",
      "@app.route('/users/<userId>/posts', methods=['GET'])\n",
      "def get_user_posts(userId):\n",
      " posts = [post for post in posts if post['userId'] == userId]\n",
      " return jsonify({'posts': posts})\n",
      "\n",
      "@app.route('/posts', methods=['POST'])\n",
      "def add_post():\n",
      " data = request.get_json()\n",
      " posts.append(data)\n",
      " return jsonify({'message': 'Post added successfully'})\n",
      "\n",
      "# Driver code \n",
      "if __name__ == '__main__':\n",
      " app.run(debug=True) \n",
      "*\n",
      "*****\n",
      "instruction: Create an AWS Lambda function in Python to process a JSON webhook request and store the data into an Amazon S3 bucket. \n",
      "*\n",
      "input: {\n",
      "  \"data\": \"example data\"\n",
      "} \n",
      "*\n",
      "output: import json\n",
      "import boto3\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    # parse the payload\n",
      "    data = json.loads(event)\n",
      "\n",
      "    # store to Amazon S3\n",
      "    s3 = boto3.client('s3')\n",
      "    s3.put_object(\n",
      "        Bucket='my-bucket',\n",
      "        Key='data.json',\n",
      "        Body=json.dumps(data)\n",
      "    )\n",
      "\n",
      "    # return a response\n",
      "    return {\n",
      "        'statusCode': 200,\n",
      "        'body': json.dumps('Data stored successfully')\n",
      "    } \n",
      "*\n",
      "*****\n",
      "instruction: Create a Python script to calculate the intersection point of two lines \n",
      "*\n",
      "input: # Line 1\n",
      "l1_m = 3\n",
      "l1_c = 5\n",
      "# Line 2\n",
      "l2_m = 7\n",
      "l2_c = 9 \n",
      "*\n",
      "output: # define a function \n",
      "def getIntersectionPoint(line1, line2): \n",
      "    # line1 (slope and intercept) \n",
      "    l1_m = line1[0]\n",
      "    l1_c = line1[1]\n",
      "  \n",
      "    # line2 (slope and intercept) \n",
      "    l2_m = line2[0]\n",
      "    l2_c = line2[1]\n",
      "    \n",
      "    x = (l2_c - l1_c) / (l1_m - l2_m)\n",
      "    y = l1_m * x + l1_c\n",
      "    return (x, y) \n",
      "  \n",
      "# given two lines\n",
      "line1 = (3, 5) \n",
      "line2 = (7, 9)\n",
      "  \n",
      "# calculate intersection point\n",
      "intersection_point = getIntersectionPoint(line1, line2)\n",
      "  \n",
      "# print the point\n",
      "print(\"Intersection point of lines is:\", intersection_point) \n",
      "*\n",
      "*****\n",
      "instruction: Develop a Python program to delete the third element of a given list. \n",
      "*\n",
      "input: list = [\"A\", \"B\", \"C\", \"D\", \"E\"] \n",
      "*\n",
      "output: # Input list\n",
      "list = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
      "\n",
      "# Deleting the third element\n",
      "list.pop(2)\n",
      "\n",
      "# Printing the list after deletion\n",
      "print(list)\n",
      "# Output: ['A', 'B', 'D', 'E'] \n",
      "*\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "l = 10\n",
    "data = a[15:15+l]\n",
    "for i in range(l):\n",
    "    for k,v in data.items():\n",
    "        if \"prompt\" in k: continue\n",
    "        print(f\"{k}: {v[i]}\",'\\n*')\n",
    "    \n",
    "    print('*'*5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    messages=[]\n",
    "    for k,v in sample.items():\n",
    "        if not v: continue\n",
    "        if k==\"instruction\" or k==\"input\":\n",
    "            messages.append({\"role\": \"user\", \"content\":v})\n",
    "        elif k==\"output\":\n",
    "            messages.append({\"role\": \"assistant\", \"content\":v})\n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds.map(process_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'text'],\n",
       "        num_rows: 14889\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'text'],\n",
       "        num_rows: 3723\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nCreate a Python program to compare two strings to determine the number of characters which are different.<|im_end|>\\n<|im_start|>user\\nNot applicable<|im_end|>\\n<|im_start|>assistant\\ndef compareStrings(str1, str2):\\n count = 0\\n \\n for i in range(min(len(str1), len(str2))):\\n if str1[i] != str2[i]:\\n count += 1\\n \\n # if the strings are different lengths, add the remaining characters from the longer string to the count\\n if len(str1) > len(str2):\\n count += len(str1) - len(str2)\\n elif len(str2) > len(str1):\\n count += len(str2) - len(str1)\\n \\n return count<|im_end|>\\n',\n",
       " '<|im_start|>user\\nCreate an AI model in Python to classify emails from the Enron Spam Dataset as either spam or non-spam.<|im_end|>\\n<|im_start|>user\\nNot applicable<|im_end|>\\n<|im_start|>assistant\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import svm\\nfrom sklearn.metrics import precision_score\\n\\n# Load the dataset\\ndf = pd.read_csv(\"spam.csv\", encoding=\\'iso-8859-1\\')\\n# Load label and data\\nX = df[\\'EmailText\\']\\ny = df[\\'Label\\']\\n\\n# Split data into train-test sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\\n\\n# Create bag of words model\\ncount_vectorizer = CountVectorizer()\\nX_train_counts = count_vectorizer.fit_transform(X_train)\\n\\n# Train the model\\nclf = svm.SVC(kernel=\\'linear\\')\\nclf.fit(X_train_counts, y_train)\\n\\n# Test the model\\nX_test_counts = count_vectorizer.transform(X_test)\\npredictions = clf.predict(X_test_counts)\\nscore = precision_score(y_test, predictions, average=\\'binary\\')\\n\\nprint(\\'Precision score of the model:\\', score)<|im_end|>\\n',\n",
       " '<|im_start|>user\\nCreate an algorithm in Python using numpy to normalize a given array of numbers.<|im_end|>\\n<|im_start|>user\\n[1, 2, 3, 4]<|im_end|>\\n<|im_start|>assistant\\nimport numpy as np\\n\\ndef normalize(arr):\\n  return (arr - np.mean(arr)) / np.std(arr)\\n\\nnormalize([1,2,3,4]) # [-1.,  0.,  1.,  2.]<|im_end|>\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=ds2['train'].select(range(100))\n",
    "b[:3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nvidia_local/opensource/smol-course/.venv/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/data/nvidia_local/opensource/smol-course/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=2,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=\"sft-python\",  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds2[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds2[\"test\"],\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 25:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>0.987215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.180900</td>\n",
       "      <td>0.972966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>0.955371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.947304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.940802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.796200</td>\n",
       "      <td>0.937213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.011900</td>\n",
       "      <td>0.933743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.929066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.737300</td>\n",
       "      <td>0.926517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.922398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.918359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.802700</td>\n",
       "      <td>0.914135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.919800</td>\n",
       "      <td>0.912018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.908432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.906072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.903248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.855100</td>\n",
       "      <td>0.902517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.899888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.898898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>0.898342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40bbf35c4eb49cbb07da9af3e08ee55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4182ac7da484f1fbd2566dc7515b33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1ae89e3e4f45788efacf03f567a659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/PavanMV/sft-python/commit/6181c726e50516c0083de7633a79b1347540ba7d', commit_message='End of training', commit_description='', oid='6181c726e50516c0083de7633a79b1347540ba7d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/PavanMV/sft-python', endpoint='https://huggingface.co', repo_type='model', repo_id='PavanMV/sft-python'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "user\n",
      "write a python code implementing linked list \n",
      "assistant\n",
      "class Node:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "\n",
      "class LinkedList:\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "\n",
      "    def append(self, data):\n",
      "        new_node = Node(data)\n",
      "        if self.head is None:\n",
      "            self.head = new_node\n",
      "            return\n",
      "        prev = self.head\n",
      "        while prev.next:\n",
      "            prev = prev.next\n",
      "        prev.next\n"
     ]
    }
   ],
   "source": [
    "prompt = \"write a python code implementing linked list \"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
